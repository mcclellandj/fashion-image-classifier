{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# __Building an optimal image classification model using a convolutional neural network and Fashion MNIST data__\n",
        "\n",
        "## __Abstract__"
      ],
      "metadata": {
        "id": "5dPNPAheKsEK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcBRNpfpZLna"
      },
      "source": [
        "The Fashion MNIST dataset is a labelled dataset which was created by Keras as an alternative to the handwritten digits MNIST dataset. It comprises images of articles of clothing and footwear with each being labelled as belonging to one of 10 classes. It is a good candidate for learning about the application of deep learning approaches to multiclass image classification problems as it a clean dataset, of reasonable size and benchmark performances are available in the usual websites."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook builds a multiclass classification model using a convolution neural network following the best practice guidance of_Deep Learning with Python_ (Chollet, 2018). It does this by using Chollet's 'Deep Learning Work Flow' approach with the aim of achieving a model which neither underfits nor overfits and is neither under-capacity nor over-capcity and has significant statistical power over a baseline model. Moreover, another model is built using the pre-trained VGG16 convnet to see how it compares to the one built from scratch."
      ],
      "metadata": {
        "id": "hnJqOHhKKUXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While striving for the best results as possible, the datasets were altered to allow all the required techniques to be undertaken. These alterations were:\n",
        "\n",
        "1. Subsetting of the data to a smaller dataset to demonstrate the power of applying data augmentation\n",
        "2. Altering the size of the Fashion MNIST images and their channel/depth dimension to allow a pre-trained convolutional network to be used\n",
        "\n",
        "Also for practical reasons the smaller datasets are perferred to reduce demanding run times that were initially experienced when using the full datasets"
      ],
      "metadata": {
        "id": "4WMelYBHKj0y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OAYN66MZLnn"
      },
      "source": [
        "## __Contents__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Set up packages\n",
        "2. Building an optimal model using convnets and the DLWP workflow\n",
        "     - Step 1: Problem definition and data\n",
        "     - Step 2: Success criteria\n",
        "     - Step 3: Evaluation protocol\n",
        "     - Step 4: Data preparation\n",
        "     - Step 5: Outperforming a baseline\n",
        "     - Step 6: Scaled up overfitting model\n",
        "     - Step 7: Optimise the overfitting model\n",
        "3. Use a pre-trained convnet to produce a model\n",
        "4. Summary and conclusions\n",
        "5. References"
      ],
      "metadata": {
        "id": "8cQb-ZAkO2n1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTWTI-SVZLnp"
      },
      "source": [
        "## __1. Set up packages__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M21ODWrLZLnr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73rDFxfhZLnv"
      },
      "source": [
        "## __2. Building an optimal model using convnets and the DLWP workflow__\n",
        "\n",
        "The development of an optimal convnet model for the problem at hand follows approach of the workflow framework in section 4.5 of 'Deep Learning With Python' (Challot, 2018).\n",
        "\n",
        "Please note that for readability superfluous cell output has been hidden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHgszAgSZLnx"
      },
      "source": [
        "### <font color='brown'>__Step 1: Problem definition and data__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Pg-ujR5BZLny"
      },
      "source": [
        "#### __a. Problem definition:__\n",
        "- Build a deep learning convnet model which can classify an image of a fashion item into one of 10 fashion categories\n",
        "- Hypotheses:\n",
        "  - the output classifications can be predicted from the input features (image pixel values)\n",
        "  - the available data is sufficiently informative\n",
        "  - The future is like the past"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is a supervised multiclass classification problem aiming to find a mapping from multiple input features to a multiclass output (classes 0-9) which minimises a categorical crossentropy loss function using the parameter update method of RMSProp. As the data is balanced in terms of class representation accuracy can be used as an evaluation metric\n",
        "- The model should have significant power in terms of beating the accuracy performance of the following baseline models:\n",
        "  - random classifier - 10% (as the data is balanced)\n",
        "  - a basic fully connected dense neural network"
      ],
      "metadata": {
        "id": "uWmMNS4NPAIS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jiu_Z9nZLn0"
      },
      "source": [
        "#### __b. Data samples and labels:__\n",
        "\n",
        "The Fashion MNIST dataset is sourced from the Keras website (Keras, 2023) and comprises four datasets:\n",
        "\n",
        "- 60,000 28 x 28 arrays representing grayscale images for training\n",
        "- 60,000 1D arrays representing fashion item labels for training\n",
        "- 10,000 28 x 28 arrays representing grayscale images for testing\n",
        "- 10,000 1D arrays representing fashion item labels for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each value in the 28 x 28 arrays is a grayscale number in the range of 0-255. \\\n",
        "Each value in the testing arrays is a number in the range of 0-9. \\\n",
        "There is no order to the data as it has been randomly shuffled.\n",
        "\n",
        "The classes are:"
      ],
      "metadata": {
        "id": "PvYyyTLtPITq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"position: absolute; left: 40px;\">\n",
        "  <table border=\"0\">\n",
        "    <tr>\n",
        "      <td>1. Ankle boots</td>\n",
        "      <td>6. Sandals</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>2. Bags</td>\n",
        "      <td>7. Shirts</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>3. Coats</td>\n",
        "      <td>8. Sneakers</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>4. Dresses</td>\n",
        "      <td>9. T-shirts/tops</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>5. Pullovers</td>\n",
        "      <td>10. Trousers</td>\n",
        "    </tr>\n",
        "  </table>\n",
        "</div>"
      ],
      "metadata": {
        "id": "t78tPmghCUo2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO-ZYj-GZLn3"
      },
      "source": [
        "The code below imports the data from Keras and then:\n",
        "- shows examples of the images\n",
        "- shows that the datasets are balanced in terms of equal representation in each of the 10 fashion classes\n",
        "- subsets the data to to a smaller data universe - one that is not too small however as to doubt the robustness of the results\n",
        "   - Training of 8,000 random samples\n",
        "   - Validation of 2,000 random samples\n",
        "   - Test of 2,000 random samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJE0pHa6ZLn4"
      },
      "source": [
        "##### Import datasets from keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMCo3eu0ZLn5"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLT24BmzZLn6"
      },
      "source": [
        "##### Check size of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlO0prT-ZLn7"
      },
      "outputs": [],
      "source": [
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy0vGjMUZLn-"
      },
      "source": [
        "##### Review the array structure one sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hUjw_F9xZLn_"
      },
      "outputs": [],
      "source": [
        "#print(train_images[2])\n",
        "#print(train_labels[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mc8nEtZZLn_"
      },
      "source": [
        "##### Review the first 25 training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qwjYabEZLoA"
      },
      "outputs": [],
      "source": [
        "# This excerpt of code was taken from the TensorFlow website  (TensorFlow, 2023)\n",
        "# Label names taken from Keras MNIST page at: https://blog.tensorflow.org/2018/04/fashion-mnist-with-tfkeras.html\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDauE5rDZLoB"
      },
      "source": [
        "##### Check if the datasets are balanced in terms of class representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vVG-O08IZLoC"
      },
      "outputs": [],
      "source": [
        "train=pd.Series(train_labels).value_counts(dropna=False).sort_index()\n",
        "test=pd.Series(test_labels).value_counts(dropna=False).sort_index()\n",
        "both=pd.concat([train,test],axis=1)\n",
        "both.plot(kind='bar',rot=0, figsize=[8,2],fontsize=7,title=['Training: No. of samples by label','Test: No. of samples by label'],legend=False,subplots=True,layout=[1,2]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMdCMc60ZLoD"
      },
      "source": [
        "##### Subset the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mBHm_PiZLoD"
      },
      "outputs": [],
      "source": [
        "train_images1=train_images[:8000]\n",
        "train_labels1=train_labels[:8000]\n",
        "\n",
        "valid_images1=train_images[58000:]\n",
        "valid_labels1=train_labels[58000:]\n",
        "\n",
        "test_images1=test_images[:2000]\n",
        "test_labels1=test_labels[:2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY0aWF4IZLoE"
      },
      "source": [
        "### <font color='brown'>__Step 2: Success criteria__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBKPPO-XZLoF"
      },
      "source": [
        "- As this is a multiclass classification problem with a balanced dataset the success criteria will be to maximise accuracy (and minimise misclassification) and that the final model beats the performance of the baseline models. An accuracy of 100% is a perfect model which discriminates the 10 classes of fashion items perfectly while an accuracy of 10% is the same as the random model and offers no statistical power to discriminate classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A more intelligent baseline model of a fully-connected dense model has been chosen over the dumb random model to highlight the power that convnets have over these types of networks for image processing problems\n",
        "- Monitoring of performance during training will also be done use accuracy on the validation data\n",
        "- If the final convnet neural network model achieves better accuracy than the baseline model then it can be considered to have better statistical power and worthy of replacing them for implementation purposes - if required"
      ],
      "metadata": {
        "id": "Tvqb-3FkPSyo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtPnzgC7ZLoF"
      },
      "source": [
        "### <font color='brown'>__Step 3: Evaluation protocol__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVrRYacNZLoG"
      },
      "source": [
        "As there are a sizeable number of training samples (8,000 for training, 2,000 for testing and 2,000 for validation) k-fold validation is not considered and the hold-out validation approach is used instead.\n",
        "\n",
        "The validation data is used for performance monitoring to aid hyperparameter tuning.\n",
        "\n",
        "The overall model build approach will use a split of:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 66.7% for model training (training)\n",
        "- 16.7% for model optimisation (validation)\n",
        "- 16.7% for model evaluation (test)\n",
        "\n",
        "The size of the splits were chosen conservatively in terms of considering the need to minimise variation in performance measures. \\\n",
        "Once hyperparameters are tuned the model will be trained on all training data (including validation) and evaluated on the unseen test data."
      ],
      "metadata": {
        "id": "mT_Rs87YPacS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU2UPmwcZLoG"
      },
      "source": [
        "### <font color='brown'>__Step 4: Data preparation__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x4ckJ1EZLoH"
      },
      "source": [
        "To build convnet neural network models with the Fashion MNIST data the following preparatory steps were undertaken (below):\n",
        "\n",
        "- Input tensors are homogeneously scaled down to float values between 0 and 1\n",
        "- The tensors are reshaped to 'image height x image width x number of image channels' (here the number of channels=1 as it is a grayscale colour scheme)\n",
        "- Class labels are one binary hot-encoded with values float formatted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGUzbDI_ZLoH"
      },
      "outputs": [],
      "source": [
        "# Reshape and standardize the inputs\n",
        "train_images2=train_images1.reshape((8000, 28, 28, 1))\n",
        "train_images2=train_images2.astype('float32') / 255\n",
        "\n",
        "test_images2=test_images1.reshape((2000, 28, 28, 1))\n",
        "test_images2=test_images2.astype('float32') / 255\n",
        "\n",
        "valid_images2=valid_images1.reshape((2000, 28, 28, 1))\n",
        "valid_images2=valid_images2.astype('float32') / 255\n",
        "\n",
        "# Reformat class labels to 0,1 with one hot-encoding\n",
        "train_labels2=to_categorical(train_labels1)\n",
        "test_labels2=to_categorical(test_labels1)\n",
        "valid_labels2=to_categorical(valid_labels1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJaVSfdtZLoI"
      },
      "source": [
        "### <font color='brown'>__Step 5: Outperforming a baseline__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh41Z-aUZLoI"
      },
      "source": [
        "To confirm whether the resulting convnet model is statistically powerful it is compared to a suitable baseline. For this project a baseline was established by building a simple fully-connected dense layer neural network. This baseline model achieved an accuracy of __75.0%__ using a simple architecture (see code below in 'a. Establish a baseline'). This will be the benchmark score to compare throughout the build of the convnet model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A low capacity convolutional neural network model was then built to beat the baseline model (see 'b. Design a low capacity convnet model'). This model shows no significant evidence of underfitting or overfitting which is good but its statistical power, while __6.6__ percentage points higher then the baseline model, is still low with accuracy of only __81.6%__ achieved."
      ],
      "metadata": {
        "id": "o22tr2rGPhtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The low capacity model is considered a starting point for which the the optimal model must out-perform. Along with the scaled up over-fitted model of step 6 below the optimised model attempts to find the balance between low capacity and/or underfitting models and a high capacity overfitting model by considering different convnet architectures and regularisation techniques."
      ],
      "metadata": {
        "id": "iC5dkD4aPjyb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU9291nrZLoJ"
      },
      "source": [
        "#### __a. Establish a baseline__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVx7dA6_ZLoJ"
      },
      "source": [
        "This baseline model comprises only one hidden dense layer of 128 units with no use of validation data for monitoring and no hyperparameter tuning or regularisation undertaken. \\\n",
        "It achieved an accuracy of __75.0%__ on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "462bOoVJZLoK"
      },
      "outputs": [],
      "source": [
        "# Reshape and standardize the inputs into fully connected dense layers\n",
        "train_images3 = train_images1.reshape((8000, 28 * 28))\n",
        "train_images3 = train_images3.astype('float32') / 255\n",
        "\n",
        "test_images3 = test_images1.reshape((2000, 28 * 28))\n",
        "test_images3 = test_images3.astype('float32') / 255\n",
        "\n",
        "# Reformat labels to 0,1 with one hot-encoding\n",
        "train_labels3 = to_categorical(train_labels1)\n",
        "test_labels3 = to_categorical(test_labels1)\n",
        "\n",
        "# Create model using softmax as output layer, compile and fit it to training data\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(28 * 28, )))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile using loss of 'categorical_crossentropy', optimizer of 'rmsprop' and evaluate on 'accuracy'\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit model to training data\n",
        "model.fit(train_images3,\n",
        "          train_labels3,\n",
        "          epochs=5,\n",
        "          batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI9I6ANWZLoL"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images3, test_labels3)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxrUEFPQZLoM"
      },
      "source": [
        "#### __b. Design a low capacity convnet model__\n",
        "\n",
        "Low capacity convnet model with only 32K parameters, built with:\n",
        "- one convolutional layer followed by one maxpooling layer to reduce the dimensionality ahead of input into the top dense layers\n",
        "- 16 filters used in the convolutional layer and a high filter patch size of 7 x 7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2 fully connected dense layers on top which provide the classifier - returning softmax probabilities for each of the 10 labels - with 16 and 10 units respectively\n",
        "- optimizer of 'rmsprop', SGD method of 'categorical crossentropy' for parameter optimizer and evaluation metric of 'accuracy' (as it's a multinomial classification problem)\n",
        "- low number of 20 epochs with a high batch rate\n",
        "- validation data used for monitoring purposes\n",
        "- no hyperparameters tuning (number of layers, number of filters, patch size, number of neurons per layer, loss rate) or regularization"
      ],
      "metadata": {
        "id": "qbUVKtwWPrvc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96b_5sziZLoN"
      },
      "source": [
        "##### Build the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTVqrjLOZLoN"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model1 = models.Sequential()\n",
        "model1.add(layers.Conv2D(16, (7, 7), activation='relu', input_shape=(28, 28, 1))) # Input shape is images of size 28 x 28 and colour is B & W so number of channels is 1\n",
        "model1.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Dense layers\n",
        "model1.add(layers.Flatten()) # Flattening of above is required before input into dense layers\n",
        "model1.add(layers.Dense(16, activation='relu'))\n",
        "model1.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Get model summary\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRtRnfW5ZLoO"
      },
      "source": [
        "##### Compile and fit the convnet to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "29PR7laGZLow"
      },
      "outputs": [],
      "source": [
        "model1.compile(optimizer='rmsprop',\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "history1=model1.fit(train_images2,\n",
        "                    train_labels2,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(valid_images2, valid_labels2),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya1PXysOZLox"
      },
      "source": [
        "##### Plot the training and validation scores by epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OpVztG4ZLoy"
      },
      "outputs": [],
      "source": [
        "def plot_results(history,y1,y2,y3,y4):\n",
        "\n",
        "    plt.figure(figsize=(15,5))\n",
        "\n",
        "    loss=history.history['loss']\n",
        "    val_loss=history.history['val_loss']\n",
        "    epochs=range(1,len(loss) + 1)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs,loss,'bo',label='Training loss')\n",
        "    plt.plot(epochs,val_loss,'b', label='Validation loss')\n",
        "    plt.ylim((y1, y2))\n",
        "    #plt.ylim((0.0, 1.0))\n",
        "    plt.title('Loss per epoch',fontsize=14)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.ylim((y3, y4))\n",
        "    #plt.ylim((0.5, 1.0))\n",
        "    plt.title('Accuracy per epoch',fontsize=14)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_results(history1,0.0,1.2,0.5,1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs5bfFrbZLoz"
      },
      "source": [
        "##### Evaluate the model on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnglPIovZLoz"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model1.evaluate(test_images2, test_labels2)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7V_49EnZLo0"
      },
      "source": [
        "### <font color='brown'>__Step 6: Scaled up overfitting model__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxOz9smwZLo1"
      },
      "source": [
        "Here a relatively larger scale model with __534K__ parameters is built with the intention to overfit. This will provide insight into where overfitting occurs (in terms of epochs) and provide an upper bound on the size of the architecture of a convnet when building the optimal model. The architecture of this model includes:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- three 2D convolutional layers\n",
        "- filters ranging from 64 to 128\n",
        "- filter patches of size 2 x 2 and 3 x 3\n",
        "- two MaxPooling layers at the bottom (to control dimensionality)\n",
        "- three dense layers\n",
        "- the last dense layer being an output layer with softmax activation function (to supply class probabilities)"
      ],
      "metadata": {
        "id": "drL7pGy6P13o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was fitted to the same data as the low-capacity model but with more epochs and a smaller batch size. It showed:\n",
        "- accuracy of __85.2%__ which is __3.6__ percentage points higher than that of the low-capacity model\n",
        "- overfitting starting to occur at epoch __12__"
      ],
      "metadata": {
        "id": "ZEfVpFXRMMEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "- As the images are only 28 x 28 x 1 there is limit in the number of max pooling layers that can be used as it reduces the dimensions quickly ahead of input into subsequent layers\n",
        "- To maintain maximum information retrieval, a small filter patch of size 2 x 2 was used on the first convolution layer"
      ],
      "metadata": {
        "id": "Rn6odt-BP8EL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIctYGzZLo2"
      },
      "source": [
        "##### Build the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQHmifU9ZLo2"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model2 = models.Sequential()\n",
        "model2.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model2.add(layers.MaxPooling2D((2, 2)))\n",
        "model2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model2.add(layers.MaxPooling2D((2, 2)))\n",
        "model2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model2.add(layers.Flatten())\n",
        "model2.add(layers.Dense(256, activation='relu'))\n",
        "model2.add(layers.Dense(64, activation='relu'))\n",
        "model2.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Get model summary\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54YrOT4XZLo4"
      },
      "source": [
        "##### Compile and fit the convnet to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "vHWTetmiZLo4"
      },
      "outputs": [],
      "source": [
        "model2.compile(optimizer='rmsprop',\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "history2=model2.fit(train_images2,\n",
        "                    train_labels2,\n",
        "                    epochs=40,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(valid_images2, valid_labels2),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl5nB6UEZLo6"
      },
      "source": [
        "##### Plot the training and validation scores by epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kP2dVW2ZLo6"
      },
      "outputs": [],
      "source": [
        "plot_results(history2,0.0,1.2,0.5,1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2VVfFtDZLo7"
      },
      "source": [
        "##### Find when the model starts to overfit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnNUPWepZLo8"
      },
      "outputs": [],
      "source": [
        "# Find lowest accuracy for validation data\n",
        "print('Minimum loss score for validation dataset:',min(history2.history['val_loss']))\n",
        "print('Minimum loss score occurs at epoch:',history2.history['val_loss'].index(min(history2.history['val_loss']))+1)\n",
        "print('')\n",
        "# Find highest accuracy for validation data\n",
        "print('Maximum accuracy score for validation dataset:',max(history2.history['val_accuracy']))\n",
        "print('Maximum accuracy score occurs at epoch:',history2.history['val_accuracy'].index(max(history2.history['val_accuracy']))+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXXFpLGgZLo9"
      },
      "source": [
        "##### Evaluate the model on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0oyla21ZLo-"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model1.evaluate(test_images2, test_labels2)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg-ZYBkIZLo_"
      },
      "source": [
        "### <font color='brown'>__Step 7: Optimise the overfitting model__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jWvLXJ5ZLpA"
      },
      "source": [
        "Here we want to find a model that ideally sits on the cusp of under-/overfitting and which is not too low-capacity or too high-capacity so it will generalise well to new data. Starting with the scaled-up model above an iterative approach is used to alter it by the following in turn:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "01. reducing the number of epochs to 11\n",
        "02. removing the first unit dense layer\n",
        "03. reducing the number of units in the dense layers\n",
        "04. undertaking L2 regularisation on the dense layers\n",
        "05. applying dropout after first dense layers\n",
        "06. apply batch normalisation to normalise the inputs into each convolutional layer\n",
        "07. adjusting the first filter to size 3 x 3\n",
        "08. applying 'padding' to get the same sized output feature maps as input feature maps allowing for more accurate analysis of images\n",
        "09. applying data augmentation to synthetically increase the volume of images for training"
      ],
      "metadata": {
        "id": "4Zou-lyaQWZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An automated grid search-type approach could be created to tune all of these hyperparameters in combination but this would be computationally very expansive. So a simple cherry-picking approach is used instead that starts with the most obvious actions such as reducing the size of the network and then subsequently applying more specialist techniques like data augmentation, rolling forward these architecture alterations to the next iteration if they help increase model performance."
      ],
      "metadata": {
        "id": "7FbKXoCsMeag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final resulting architecture actually increases in terms of parameters to __1.8M__ due to the flattened layer (ahead of input into the classifier) being considerably larger than previously due to the changes made to the convolutional layers.\n",
        "    \n",
        "When this model is trained on the full training data and evaluated on the test hold-out data it achieves a relatively low loss of __0.394__ and shows little evidence of underfitting or overfitting. It achieves an accuracy of __87.5%__ which easily surpasses the __75.0%__ of the baseline model by __12.5__ percentage points"
      ],
      "metadata": {
        "id": "PkRX4w1DMn1_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70L2s-VZLpB"
      },
      "source": [
        "#### __a. Reduce underfitting by applying architecture alterations__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ7am6h1ZLpB"
      },
      "source": [
        "##### __Create a function to compile, fit and evaluate of each model iteration__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEHeXcm1ZLpC"
      },
      "outputs": [],
      "source": [
        "def model_run_eval(model):\n",
        "\n",
        "    # Compile\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # fit\n",
        "    history=model.fit(train_images2,\n",
        "                      train_labels2,\n",
        "                      epochs=11,\n",
        "                      batch_size=128,\n",
        "                      validation_data=(valid_images2, valid_labels2),\n",
        "                      verbose=1)\n",
        "\n",
        "    # Plot the training and validation scores by epoch\n",
        "    plot_results(history,0.0,1.2,0.5,1.0)\n",
        "\n",
        "    # Evaluate the model on test dataset\n",
        "    test_loss, test_acc = model.evaluate(test_images2, test_labels2)\n",
        "    print('Test loss:',test_loss)\n",
        "    print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Atq80phZLpD"
      },
      "source": [
        "##### __Iteration 1 - reducing the number of epochs to 11__\n",
        "\n",
        "Results show improvement versus the high capacity model so roll forward this architecture\\\n",
        "Test loss: 0.40828195214271545 vs 0.43423593044281006\\\n",
        "Test accuracy: 0.8575000166893005 vs 0.8519999980926514\\\n",
        "No under/overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "YVNljbxLZLpD"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPU4OHutZLpE"
      },
      "source": [
        "##### __Iteration 2 - remove the first unit dense layer__\n",
        "\n",
        "Results show no improvement so roll back to iteration 1 architecture\\\n",
        "Test loss: 0.429411262273788453 \\\n",
        "Test accuracy:  0.8450000286102295 \\\n",
        "No under/overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "AjhxNJm5ZLpF"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdHUbcUUZLpG"
      },
      "source": [
        "##### __Iteration 3 - reduce the number of units in the dense layers__\n",
        "\n",
        "Results show no improvement so roll back to iteration 1 architecture\\\n",
        "Test loss: 0.6133295297622681 \\\n",
        "Test accuracy:  0.8019999861717224 \\\n",
        "Overfitting evident"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "8_pbUBj4ZLpG"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgydiEwZZLpH"
      },
      "source": [
        "##### __Iteration 4 - undertake L2 regularisation on the dense layers__\n",
        "\n",
        "Results show no improvement so roll back to iteration 1 architecture\\\n",
        "Test loss: 0.5948675870895386 \\\n",
        "Test accuracy: 0.809499979019165  \\\n",
        "No under/overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "QBUkDA6cZLpI"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lmWMfcSZLpI"
      },
      "source": [
        "##### __Iteration 5 - apply dropout after dense layers__\n",
        "\n",
        "Results show improvement so roll forward this architecture\\\n",
        "Test loss: 0.4034961462020874  \\\n",
        "Test accuracy: 0.8604999780654907    \\\n",
        "No significant under/overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "v4RYJvR1ZLpJ"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYc9P0bbZLpK"
      },
      "source": [
        "##### __Iteration 6 - apply batch normalisation to normalise the inputs into each convolutional layer__\n",
        "\n",
        "Results show improvement on accuracy but the training model does not generalise well with significant differences between training and validation in loss and accuracy scores over the epochs so roll back to iteration 5 architecture\\\n",
        "Test loss: 0.6232907772064209   \\\n",
        "Test accuracy: 0.8579999804496765     \\\n",
        "Extremely high loss for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "kUyFV2KOZLpL"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9phv_5dZLpM"
      },
      "source": [
        "##### __Iteration 7 - adjusting the first filter to size 3 x 3__\n",
        "\n",
        "Results show noticeable improvement so roll forward this architecture\\\n",
        "Test loss: 0.3863731920719147\\\n",
        "Test accuracy: 0.8675000071525574\\\n",
        "No significant under/overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "3ijsIXEUZLpN"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI7kaiSDZLpO"
      },
      "source": [
        "##### __Iteration 8 - applying 'padding' to get the same sized output feature maps as input feature maps allowing for more accurate analysis of images__\n",
        "\n",
        "Results show significant improvement so roll forward this architecture\\\n",
        "Test loss: 0.3321758210659027\\\n",
        "Test accuracy: 0.8889999985694885\\\n",
        "No significant under/overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Ybep-pSQZLpO"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(28, 28, 1),padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model_run_eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWH1Al_FZLpP"
      },
      "source": [
        "##### __Iteration 9 - applying data augmentation to synthetically increase the volume of images for training__\n",
        "\n",
        "Results show no improvement so roll back to iteration 8 architecture\\\n",
        "Test loss: 0.5662176609039307\\\n",
        "Test accuracy: 0.7825000286102295\\\n",
        "Significant underfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "1o3aCXCVZLpQ"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(28, 28, 1),padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the convnet\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Create generator with transformation steps\n",
        "datagen = ImageDataGenerator(rotation_range=40,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True)\n",
        "\n",
        "# Fit transformer to the training data\n",
        "datagen.fit(train_images2)\n",
        "\n",
        "# Fit the model to the augmented data\n",
        "# In each epoch all of the original images in training are transformed as per the ImageDataGenerator and used for training\n",
        "# The number of images in each epoch is equal to the number of original images\n",
        "# As training has 8,000 samples and batch size is set to 32 then steps per epoch will to be set to 250\n",
        "# As validation has 2,000 samples and batch size is set to 32 then validation_steps will be set to 63\n",
        "history = model.fit(datagen.flow(train_images2, train_labels2, batch_size=32),\n",
        "                    steps_per_epoch=250,\n",
        "                    epochs=11,\n",
        "                    validation_data=(valid_images2, valid_labels2),\n",
        "                    validation_steps=63,\n",
        "                    verbose=2)\n",
        "\n",
        "# Plot the training and validation scores by epoch\n",
        "plot_results(history,0.0,1.2,0.5,1.0)\n",
        "\n",
        "# Evaluate the model on test dataset\n",
        "test_loss, test_acc = model.evaluate(test_images2, test_labels2)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k_eWdQ8ZLpR"
      },
      "source": [
        "#### __b. Evaluate final architecture on full training data__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFnYQTPWZLpR"
      },
      "source": [
        "##### __Get full training data and preprocess__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eauZ4Yz6ZLpS"
      },
      "outputs": [],
      "source": [
        "# Get full training data\n",
        "train_images_all = np.concatenate((train_images1, valid_images1))\n",
        "train_labels_all = np.concatenate((train_labels1, valid_labels1))\n",
        "\n",
        "# Reshape and standardize the training inputs\n",
        "train_images_all1=train_images_all.reshape((10000, 28, 28, 1))\n",
        "train_images_all2=train_images1.astype('float32') / 255\n",
        "\n",
        "# Reformat training class labels to 0,1 with one hot-encoding\n",
        "train_labels_all=to_categorical(train_labels_all)\n",
        "\n",
        "print(train_images_all.shape)\n",
        "print(train_labels_all.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt3AL9BTZLpT"
      },
      "source": [
        "##### __Train using finalised architecture and evaluate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXI64dJGZLpU"
      },
      "outputs": [],
      "source": [
        "# Convolutional layers\n",
        "model_final = models.Sequential()\n",
        "model_final.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(28, 28, 1),padding='same'))\n",
        "model_final.add(layers.MaxPooling2D((2, 2)))\n",
        "model_final.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
        "model_final.add(layers.MaxPooling2D((2, 2)))\n",
        "model_final.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
        "\n",
        "# Dense layers\n",
        "model_final.add(layers.Flatten())\n",
        "model_final.add(layers.Dense(256, activation='relu'))\n",
        "model_final.add(layers.Dropout(0.25))\n",
        "model_final.add(layers.Dense(64, activation='relu'))\n",
        "model_final.add(layers.Dropout(0.25))\n",
        "model_final.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Get model summary\n",
        "print(model_final.summary())\n",
        "\n",
        "# Compile\n",
        "model_final.compile(optimizer='rmsprop',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# fit\n",
        "history_final=model_final.fit(train_images2,\n",
        "                              train_labels2,\n",
        "                              epochs=11,\n",
        "                              batch_size=128,\n",
        "                              verbose=1)\n",
        "\n",
        "# Plot the loss and accuracy scores by epoch\n",
        "\n",
        "# Get scores and epochs\n",
        "acc=history_final.history['accuracy']\n",
        "loss=history_final.history['loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Create plot\n",
        "fig, ax1 = plt.subplots(figsize=(15, 5), dpi=80)\n",
        "ax2 = ax1.twinx()\n",
        "plot1=ax1.plot(epochs, acc, 'b-', label='Accuracy')\n",
        "plot2=ax2.plot(epochs, loss, 'r-', label='Loss')\n",
        "plt.title('Optimised model - full training data - accuracy and loss per epoch')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax1.set_xticks(np.arange(1, len(acc)+1, step=1))\n",
        "# Create legend\n",
        "plots=plot1+plot2\n",
        "labels= [l.get_label() for l in plots]\n",
        "ax1.legend(plots, labels, loc='best', frameon=False)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on test dataset\n",
        "test_loss, test_acc = model_final.evaluate(test_images2, test_labels2)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkGMTH61ZLpV"
      },
      "source": [
        "##### __Save final model__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baRkAtEwZLpW"
      },
      "outputs": [],
      "source": [
        "model_final.save('fashion_mnist_final.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJNSZ27fZLpW"
      },
      "source": [
        "## __3. Use a pre-trained convnet to produce a model__\n",
        "\n",
        "Instead of creating a convnet from scratch on a small dataset of images transfer learning can be undertaken by using a convnet already trained on an extraordinarily large image dataset. Unfortunately there are no pre-trained convnets available for small grayscale images so adjustments had to be made to the Fashion MNIST images before applying them to the Keras pre-trained VGG16 convnet. VGG16 is trained on the ImageNet dataset which contains 1.4 million images associated with 1,000 different classes of everday objects (animate and inanimate)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pre-trained VGG16 convnet we can reuse its convolutional layers (including its weights and biases) and add a new and relevant densely-connected classifier on top of it. The pre-trained classifier of VGG16 was trained on ImageNet classes (1,000 labels) and so needs to be trained on the Fashion MNIST classes. To illustrate the differences in the classes, 'T-shirt' apears in the Fashion MNIST class of 'T-shirt/top' while in ImageNet it is classified as 'jersey, T-shirt, tee shirt' (WekaDeeplearning4j, 2023)."
      ],
      "metadata": {
        "id": "Avdi2zBdM1SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Fashion MNIST data will be run through the convolutional base and the resulting features will train the new classifier from scratch using the classes of Fashion MNIST. The fully connected dense layers will simply comprise one input layer and one softmax output layer."
      ],
      "metadata": {
        "id": "aCapqpE2M1qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To account for the problem of the Fashion MNIST images being being incompatible with VGG16, the shape of the input tensors needed to be transformed from 28 x 28 x 1 to 32 x 32 x 3. Guidance on how to convert of a one grayscale channel to three (fake) RGB channels was taken from an example on stackoverflow (stackoverflow, 2023). Unfortunately, while edges are retained a lot of details within the images are removed so expectations of this achieving good results were low."
      ],
      "metadata": {
        "id": "G9CCvAokM15w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As suspected, due to the image transformation, the use of a pretrained convnet achieved only __83.6%__ accuracy for test accuary with the loss and accuracy scores remaining fairly constant across the epochs - this maybe a result of the poor image quality. Applying fine tuning on the the top layers of the convolutional base and the dense layers - to make the higher level abstract representations more relevant for the problem at hand (Chollet, 2018) - resulted in a slightly higher test accuracy score of: __84.5%__ which is noticeably lower than the test accuracy for the optimal model (__87.5__)."
      ],
      "metadata": {
        "id": "2nlE7ds9NML3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGxMx2a9ZLpX"
      },
      "source": [
        "#### __a. Data preparation__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgpZK7bkZLpY"
      },
      "source": [
        "##### Create RGB channels for each image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8OumjTdZLpY"
      },
      "outputs": [],
      "source": [
        "# Original data\n",
        "print(train_images1.shape)\n",
        "print(valid_images2.shape)\n",
        "print(test_images2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQMy64jcZLpZ"
      },
      "outputs": [],
      "source": [
        "# Expand channels\n",
        "train_images_rgb = np.repeat(train_images1[..., np.newaxis], 3, -1)\n",
        "valid_images_rgb = np.repeat(valid_images1[..., np.newaxis], 3, -1)\n",
        "test_images_rgb = np.repeat(test_images1[..., np.newaxis], 3, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cir-GY-8ZLpZ"
      },
      "outputs": [],
      "source": [
        "print(train_images_rgb.shape)\n",
        "print(valid_images_rgb.shape)\n",
        "print(test_images_rgb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjtuStS1ZLpa"
      },
      "source": [
        "##### Increase the size of the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-6uW7EOZLpa"
      },
      "outputs": [],
      "source": [
        "train_images_rgb1 = tf.image.resize(train_images_rgb, [32,32])\n",
        "valid_images_rgb1 = tf.image.resize(valid_images_rgb, [32,32])\n",
        "test_images_rgb1 = tf.image.resize(test_images_rgb, [32,32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3zf_4q-ZLpb"
      },
      "outputs": [],
      "source": [
        "print(train_images_rgb1.shape)\n",
        "print(valid_images_rgb1.shape)\n",
        "print(test_images_rgb1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT7Oh9QeZLpb"
      },
      "source": [
        "##### Check a sample image before vs after transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mADyBfPuZLpc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.subplots_adjust(hspace=0.40)\n",
        "\n",
        "plt.subplot(3,2,1)\n",
        "plt.imshow(train_images1[0], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels1[0]])\n",
        "\n",
        "plt.subplot(3,2,2)\n",
        "plt.imshow(train_images_rgb1[0], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels1[0]])\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "plt.imshow(train_images1[1], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels1[1]])\n",
        "\n",
        "plt.subplot(3,2,4)\n",
        "plt.imshow(train_images_rgb1[1], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels1[1]])\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "plt.imshow(train_images1[2], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels1[2]])\n",
        "\n",
        "plt.subplot(3,2,6)\n",
        "plt.imshow(train_images_rgb1[2], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels1[2]])\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ucGmdUZLpd"
      },
      "source": [
        "#### __b. Import pretrained VGG16 convnet__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4GwAlb2ZLpd"
      },
      "outputs": [],
      "source": [
        "# Extract the weights of VGG16 but exclude the densely-connected classifer\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(32,32,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "k_rDjo0hZLpe"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjBzrkyLZLpf"
      },
      "source": [
        "#### __c. Create network__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INBRrYWQZLpf"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(conv_base)\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Get model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkA1vIyCZLpg"
      },
      "source": [
        "#### __d. Train model and evaluate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZOM-B2NZLpg"
      },
      "outputs": [],
      "source": [
        "# Freeze the convolutional layers so as not to be retrained\n",
        "conv_base.trainable=False\n",
        "\n",
        "# Compile\n",
        "# Here Chollet advises 'using a very small learning rate to limit the magnitude of the modifications being made to the layers being tuned. Updates that are too\n",
        "# large may harm their representations.'\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=1e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit using same number of epochs as per optimal model created above\n",
        "history=model.fit(train_images_rgb1,\n",
        "                  train_labels2,\n",
        "                  epochs=11,\n",
        "                  batch_size=128,\n",
        "                  validation_data=(valid_images_rgb1, valid_labels2),\n",
        "                  verbose=1)\n",
        "\n",
        "# Plot the training and validation scores by epoch\n",
        "plot_results(history,0.0,0.3,0.5,1.0)\n",
        "\n",
        "# Evaluate the model on test dataset\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb1, test_labels2)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BXTQb4PZLph"
      },
      "source": [
        "#### __e. Fine tune the block 5 convolutional and dense layers and evaluate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmnGXCh6ZLpi"
      },
      "outputs": [],
      "source": [
        "# This code is taken from Deep Learning with Python (Chollett, 2018)\n",
        "\n",
        "# Freeze all the layers up to block 5 in the convolutional layers\n",
        "\n",
        "# Firstly, unfreeze the convolutional layers (previously frozen above)\n",
        "conv_base.trainable = True\n",
        "\n",
        "# Freeze the required layers\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=1e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit using same number of epochs as per optimal model created above\n",
        "history=model.fit(train_images_rgb1,\n",
        "                  train_labels2,\n",
        "                  epochs=11,\n",
        "                  batch_size=128,\n",
        "                  validation_data=(valid_images_rgb1, valid_labels2),\n",
        "                  verbose=1)\n",
        "\n",
        "# Plot the training and validation scores by epoch\n",
        "plot_results(history,0.0,0.3,0.5,1.0)\n",
        "\n",
        "# Evaluate the model on test dataset\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb1, test_labels2)\n",
        "print('Test loss:',test_loss)\n",
        "print('Test accuracy:',test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYrr0Ut1ZLpj"
      },
      "source": [
        "## __4. Summary and conclusions__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_cp9_7eZLpk"
      },
      "source": [
        "This project set out to build a multiclass classification model for image data using neural networks design for image processing - convolutional networks. Using a subset of the full dataset an optimal model was created which did not underfit or overfit the data. Cherry picking key hyperparameters was valuable in optimising during the model build, namely:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- reducing the number of epochs\n",
        "- adding drop regularisation\n",
        "- increasing the filter patch size slightly\n",
        "- incorporating padding to get the most information for output feature maps"
      ],
      "metadata": {
        "id": "eJ6ZuRXNNcSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing the number of layers and units, utilising synthetic images via data augmentation, and applying other regularization techniques such as L1, batch normalisation did not help with reducing overfitting."
      ],
      "metadata": {
        "id": "2Z6ffewoOJMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model achieved an accuracy score of __87.5%__ which easily passed the evaluation criterion of beating the baseline model - by __12.5__ percentage points.  While I'm please with how the workflow process was used to obtain a final model with significant statistical power based on a small dataset, the accuracy score is considerably less than other benchmarks available online where scores of mid to high 90s are being achieved for the Fashion MNIST dataset. It remains to be seen what benchmarks are available for builds involving small subsets of the Fashion MNIST data. It's key advantage is processing speed in the absence of GPU availability. But also I wanted to see how effective data augmentation could be."
      ],
      "metadata": {
        "id": "q1IqAHxmNcgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The use of a pre-trained convnet (VGG16) did not perform well which may be due to the hack applied to the input data images. The fine tuning of the model achieved from using the pretrained convnet did however improve the untuned model, but not enough to beat the optimal model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fga5WNmVNcq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given hindsight, If this project was to be repeated I would consider these aspects:\n",
        "- gain access to a GPU so can process full datasets and still undertake data augmentations. I am sure better results would be achieved with more data\n",
        "- use a different dataset so that a pre-trained convnet could be used more successfully\n",
        "- use k-fold validation for evaluating model performance\n",
        "- include more visual evaluation figures like confusion matrices"
      ],
      "metadata": {
        "id": "bbq9fAJjNcy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxX0mxfqZLpl"
      },
      "source": [
        "## __5. References__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTFJBwpZLpl"
      },
      "source": [
        "1. Chollet, F. _Deep learning with Python_, first edition 2018\n",
        "2. TensorFlow, _Basic classification: Classify images of clothing_, found at https://www.tensorflow.org/tutorials/keras/classification (2023)\n",
        "2. Keras, _Fashion MNIST dataset, an alternative to MNIST_, found at https://keras.io/api/datasets/fashion_mnist/, 2023\n",
        "3. WekaDeeplearning4j, _IMAGENET 1000 Class List_, found at https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/, 2023\n",
        "4. stackoverflow, _How can I use a pre-trained neural network with grayscale images?_, found at https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images, 2023"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}